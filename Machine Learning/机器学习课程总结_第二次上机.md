# 机器学习课程总结

每次上课的内容：

1. 介绍机器学习：什么是机器学习，学习一种共性东西，大脑存储的是内在规律（隐式），从客体中总结规律（显式），无法通过简洁公式表达规律，所以通过机器学习，期望（利用**学习机**）从复杂函数将规律表达出来。

   机器学习发展史：科学发展的必然性

2. 最小二乘法：用机器学习的方向理解，

   - 最小二乘法如何计算
   - 最小二乘法概率理解，对数据的假设，每个数据服从一个独立同分布，一个线性函数加上一个标准正态分布。当出现过拟合时，加入正则项，仍可通过概率方式理解，概率框架为**最大后验估计**，将参数 $w$ 求解转化为随机变量参数。
   - 代码：混合Gauss，try？最小二乘，加入正则项如何求解？

3. K-means，证明，计算；高层次理解，从数据挖掘变为机器学习算法，如何对一个新的点进行聚类？利用**数学函数**表达出来，这个在预测中存在，且在训练时存在（隐式），是存在一个**学习机**的（不考试）

4. 神经网络，显然是个机器学习算法，不易把握机器学习学科特性，关键就是**BP算法**，求每个节点上的梯度，考试时只需将方向传播的基本形式推导出来即可。

5. 主层次分析法（PCA），内在都存在一个**学习机**，如何从高维将维到低维空间，分为三个层次：

   - 最大方差，先找第一维再找第二维……（直接降维，考试，见山是山，见水是水）
   - 低维与高维数据，如何足够接近，学习如何利用小样本计算大数据量，降低计算复杂度（间接降维，考试，见山不是山，见山不是水）
   - 如何对新的数据进行将维，利用已有数据作为基底进行将维，并且进行解码，所以也是在训练过程中存在，也是在预测中存在，所以也是一个**学习机**模型。（不考，见山还是山，见水还是水）

6. 支撑向量机（SVM），线性支撑向量机，是一个凸优化问题；关键是可以通过对偶优化，证明了SVM存在支撑向量，也就是通过数学方法解释了该算法。
   - （非）线性支撑向量机优化推导，理解（考试）
   - 统计学习理论（考的少）
   - 支撑向量如何起到作用（如果用代码实现SVM）

考试中有发挥题、未来发展，代码可以用MATLAB直接调用，但需要解释清楚每行代码。

> 还有一次上机讲解和答疑课