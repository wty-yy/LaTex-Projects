\begin{thebibliography}{11}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{#1}
\expandafter\ifx\csname urlstyle\endcsname\relax\relax\else
  \urlstyle{same}\fi
\providecommand{\href}[2]{\url{#2}}
\providecommand{\doi}[1]{\href{https://doi.org/#1}{#1}}

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{DQN}
Mnih~V, Kavukcuoglu~K, Silver~D, et~al.
\newblock Human-level control through deep reinforcement
  learning\allowbreak[J].
\newblock Nature, 2015, 518\penalty0 (7540):\penalty0 529-533.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{AlphaGo}
Silver~D, Huang~A, Maddison~C~J, et~al.
\newblock Mastering the game of go with deep neural networks and tree
  search\allowbreak[J].
\newblock nature, 2016, 529\penalty0 (7587):\penalty0 484-489.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{AlphaStar}
Vinyals~O, Babuschkin~I, Czarnecki~W~M, et~al.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning\allowbreak[J].
\newblock Nature, 2019, 575\penalty0 (7782):\penalty0 350-354.

\bibitem[Berner et~al.(2019)Berner, Brockman, Chan, Cheung, D{\k{e}}biak,
  Dennison, Farhi, Fischer, Hashme, Hesse, et~al.]{OpenAIFive}
Berner~C, Brockman~G, Chan~B, et~al.
\newblock Dota 2 with large scale deep reinforcement learning\allowbreak[J].
\newblock arXiv preprint arXiv:1912.06680, 2019.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{PPO}
Schulman~J, Wolski~F, Dhariwal~P, et~al.
\newblock Proximal policy optimization algorithms\allowbreak[J].
\newblock arXiv preprint arXiv:1707.06347, 2017.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and Levine]{CQL}
Kumar~A, Zhou~A, Tucker~G, et~al.
\newblock Conservative q-learning for offline reinforcement
  learning\allowbreak[J].
\newblock Advances in Neural Information Processing Systems, 2020, 33:\penalty0
  1179-1191.

\bibitem[Chen et~al.(2021)Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel,
  Srinivas, and Mordatch]{DT}
Chen~L, Lu~K, Rajeswaran~A, et~al.
\newblock Decision transformer: Reinforcement learning via sequence
  modeling\allowbreak[J].
\newblock Advances in neural information processing systems, 2021, 34:\penalty0
  15084-15097.

\bibitem[Redmon et~al.(2016)Redmon, Divvala, Girshick, and Farhadi]{YOLO}
Redmon~J, Divvala~S, Girshick~R, et~al.
\newblock You only look once: Unified, real-time object
  detection\allowbreak[C]//\allowbreak
Proceedings of the IEEE conference on computer vision and pattern recognition.
\newblock 2016: 779-788.

\bibitem[Zheng et~al.(2020)Zheng, Wang, Liu, Li, Ye, and Ren]{CIOU}
Zheng~Z, Wang~P, Liu~W, et~al.
\newblock Distance-iou loss: Faster and better learning for bounding box
  regression\allowbreak[C]//\allowbreak
Proceedings of the AAAI conference on artificial intelligence: volume~34.
\newblock 2020: 12993-13000.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, Sutskever,
  et~al.]{GPT}
Radford~A, Narasimhan~K, Salimans~T, et~al.
\newblock Improving language understanding by generative
  pre-training\allowbreak[J].
\newblock 2018.

\bibitem[Shang et~al.(2022)Shang, Kahatapitiya, Li, and Ryoo]{StARformer}
Shang~J, Kahatapitiya~K, Li~X, et~al.
\newblock Starformer: Transformer with state-action-reward representations for
  visual reinforcement learning\allowbreak[C]//\allowbreak
European conference on computer vision.
\newblock Springer, 2022: 462-479.

\end{thebibliography}
