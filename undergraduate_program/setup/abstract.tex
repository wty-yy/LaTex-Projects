% !Mode:: "TeX:UTF-8" 

%-----------------------------------------------------------------------------------------
% 中文摘要
\clearpage
\titlespacing{\chapter}{0pt}{0mm}{5mm}
% \Abstract{摘\quad 要}{Abstract (In Chinese)}
\BiAppendixChapter{摘\quad 要}{Abstract (In Chinese)}
% \chapter{摘\quad 要}

\defaultfont
本文提出了一种基于非嵌入式离线强化学习的训练策略，通过图像信息输入在即时策略游戏皇室战争（Clash Royale）中实现自主博弈对局。
本文结合当前目标识别与光学文本识别的前沿算法对图像信息进行特征提取，并使用离线强化学习算法进行训练，
实现了移动设备上的实时图像获取、图像感知特征融合、智能体决策及设备控制，与对手进行实时对局，并能够战胜游戏中的内置AI。

首先，本文设计了一种高效制作切片数据集的方法，在此基础上制作了包含$4654$张切片的数据集，涵盖了游戏中的全部部队、法术、防御塔等总计$150$个类别。
本文提出了一种生成目标识别数据集的算法，能够模拟真实对局场景生成带标签图像，用于目标识别模型训练。
本文从视频流中截取$6939$张图像，制作了包含$116878$个目标框的目标识别数据集作为验证集\footnote{图像数据集：\url{https://github.com/wty-yy/Clash-Royale-Detection-Dataset}\hfill}。
通过生成式数据集训练出的目标识别模型在验证集中表现出良好的泛化性，在$50\%$交并比阈值下达到了$80.9\%$的召回率。

其次，基于目标识别、光学文本识别及图像分类模型的输出特征，本文设计了图像感知融合算法，能够将感知特征预处理为模型的输入特征。
该算法基于视频数据中的上下帧信息及单位的关联信息恢复漏识别目标，
排除干扰性特征，优化模型输入特征。

最后，在决策模型方面，本文制作了包含$105$个回合、总共$113981$帧的专家数据集\footnote{专家数据集：\url{https://github.com/wty-yy/Clash-Royale-Replay-Dataset}\hfill}，
在智能体不与环境交互的条件下进行训练。本文从架构及预测目标两个方面对离线强化学习模型进行了改进，将难以学习的离散动作序列转化为连续动作序列，
相比传统模型性能分别提高了$24\%$和$37\%$。

本文的全部代码均已开源\footnote{全部代码：\url{https://github.com/wty-yy/katacr}}。

\vspace{\baselineskip}
% \noindent{\textbf{关键词：}} 非嵌入式游戏AI；生成式目标识别数据集；离线强化学习
\noindent{\textbf{关键词：}} 目标识别；光学文本识别；强化学习

%-----------------------------------------------------------------------------------------
% 英文摘要
\clearpage
%\phantomsection
\setcounter{footnote}{0}
\markboth{Abstract}{Abstract}

\titlespacing{\chapter}{0pt}{0mm}{5mm}
% \chapter*{ABSTRACT}
\BiAppendixChapter{ABSTRACT}{}
% \chapter{Abstract}

\noindent This thesis proposes a training strategy based on non-embedded offline reinforcement learning to achieve autonomous gameplay in the real-time strategy game Clash Royale through image information input.
The thesis integrates cutting-edge algorithms for object recognition and optical character recognition to extract features from image information and uses offline reinforcement learning algorithms for training.
This enables real-time image acquisition, perception fusion, agent decision-making, and device control on mobile devices, allowing real-time matches against opponents and defeating the built-in AI in the game.

\noindent Firstly, the thesis designs an efficient method for creating a sliced dataset and produces a dataset containing 4654 slices, covering all troops, spells, and towers in the game, totaling 150 categories. The thesis proposes an algorithm to generate an object recognition dataset that can simulate real-game scenarios to
generate labeled images for training object recognition models. The thesis extracts 6939 images from video streams, creating an object recognition validation dataset containing 116878 bounding boxes\footnote{Image dataset: \url{https://github.com/wty-yy/Clash-Royale-Detection-Dataset}\hfill}. The object recognition model trained with the generative dataset shows good generalization in the validation set, achieving an 80.9\% recall rate at a 50\% Intersection over Union (IoU) threshold.

\noindent Secondly, based on the output features of object recognition, optical character recognition, and image classification models, the thesis designs a perception fusion algorithm that preprocesses perception features into model input features. The algorithm recovers missed targets based on inter-frame information and unit association information in the video data, eliminating interfering features and optimizing the model input features.

\noindent Finally, in terms of the decision-making model, the thesis creates an expert dataset containing 105 rounds and a total of 113981 frames\footnote{Expert dataset: \url{https://github.com/wty-yy/Clash-Royale-Replay-Dataset}\hfill}, training the agent without interacting with the environment. The thesis improves traditional offline reinforcement learning models from both architecture and prediction objectives, transforming hard-to-learn discrete action sequences into continuous action sequences, improving performance by 24\% and 37\% compared to traditional models, respectively.

\noindent All code in this thesis has been open-sourced\footnote{All code: \url{https://github.com/wty-yy/katacr}}.

\vspace{\baselineskip}
\noindent{\textbf{Keywords:}} Object recognition; Optical character recognition; Reinforcement learning
% \noindent{\textbf{Keywords:}} Non-embedded game AI; Generative object detection dataset; Offline reinforcement learning


\titlespacing{\chapter}{0pt}{-6mm}{5mm}
\clearpage{\pagestyle{empty}\cleardoublepage}
